{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Supreme Court Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraries\n"
      ],
      "metadata": {
        "id": "c_YB6O3VqTxn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4U4Ec1AcdNa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "#import facts_preprocessing\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import json\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload cleaned CSV file, source: https://github.com/DavidOB1/supreme-court-nlp/"
      ],
      "metadata": {
        "id": "mMEI3v-jqXfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"clean_data.csv\",encoding=\"Windows-1252\").dropna()"
      ],
      "metadata": {
        "id": "YQbBwXAZreI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_for_model = pd.DataFrame()"
      ],
      "metadata": {
        "id": "ziFzWBvQypJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define some variables"
      ],
      "metadata": {
        "id": "vcjceq_2q-8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "embedding_dim = 100\n",
        "# 1000 for character level\n",
        "seq_length = 250\n",
        "# 75 for character level\n",
        "max_features = 20000\n",
        "embedding_matrix = np.zeros((max_features, embedding_dim))"
      ],
      "metadata": {
        "id": "nEF8-fHdq8Bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras Tokenizer\n"
      ],
      "metadata": {
        "id": "Bw3d-2H6rJyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_with_keras(text_data, char_level = False):\n",
        "  global max_features\n",
        "  tokenizer = Tokenizer(num_words=max_features, split=' ',char_level=char_level)\n",
        "  tokenizer.fit_on_texts(text_data.values)  \n",
        "  max_features = tokenizer.num_words\n",
        "  #X = tokenizer.texts_to_sequences(text_data.values)\n",
        "  #X = pad_sequences(X, padding = \"post\",truncating=\"post\",maxlen = seq_length)\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "aCRFMmI_rPXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the data"
      ],
      "metadata": {
        "id": "tNviWq-6rSak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(use_keras = True):\n",
        "  \n",
        "  if (use_keras):\n",
        "    vectorizer = tokenize_with_keras(data[\"facts\"],char_level = True)  \n",
        "    data_for_model[\"facts\"] = vectorizer.texts_to_sequences(data[\"facts\"].values)\n",
        "  else:\n",
        "    vectorizer = facts_preprocessing.create_vectorization_model(data[\"facts\"])\n",
        "    data_for_model[\"facts\"] = data[\"facts\"].apply(lambda x: facts_preprocessing.vectorize_string(x,vectorizer))\n",
        "    global max_features\n",
        "    max_features = vectorizer.num_words\n",
        "  #data[\"first_party\"] = data[\"first_party\"].apply(lambda x: facts_preprocessing.vectorize_string(x,vectorizer))\n",
        "  #data[\"second_party\"] = data[\"second_party\"].apply(lambda x: facts_preprocessing.vectorize_string(x,vectorizer))\n",
        "  data_for_model[\"first_party_won\"] = data[\"first_party_won\"].apply(lambda x: 1 if x == True else 0)\n",
        "  data_for_model[\"ideologies\"] = data[\"ideologies\"].apply(lambda x: json.loads(x))\n",
        "\n",
        "  return data_for_model, vectorizer"
      ],
      "metadata": {
        "id": "-tMDiExurXVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we're using Gensim, we need to make a matrix of vectors for use in the embedding layer"
      ],
      "metadata": {
        "id": "0c46_7VmvYgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_embedding(vectorizer):\n",
        "  hits = 0\n",
        "  misses = 0\n",
        "  global max_features\n",
        "  max_features = vectorizer.num_words\n",
        "  global embedding_matrix\n",
        "  embedding_matrix = np.zeros((max_features, embedding_dim))\n",
        "  \n",
        "  for word, i in vectorizer.wv.key_to_index.items():\n",
        "      embedding_vector = vectorizer.wv[word]\n",
        "      if embedding_vector is not None:\n",
        "          # Words not found in embedding index will be all-zeros.\n",
        "          # This includes the representation for \"padding\" and \"OOV\"\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "          hits += 1\n",
        "      else:\n",
        "          misses += 1\n",
        "  print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "metadata": {
        "id": "C6qDxrYwveiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Organize data into vectors for use in the model"
      ],
      "metadata": {
        "id": "ZNLbrFN7sago"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_complete_data():\n",
        "  global data_for_model\n",
        "  data_for_model = data_for_model.dropna() \n",
        "  X = pad_sequences(data_for_model[\"facts\"],maxlen=seq_length,padding=\"post\", truncating=\"post\")\n",
        "  #FP = pad_sequences(data[\"first_party\"],maxlen=20,padding=\"post\", truncating=\"post\")\n",
        "  #SP = pad_sequences(data[\"second_party\"],maxlen=20,padding=\"post\", truncating=\"post\")\n",
        "  I = pad_sequences(data_for_model[\"ideologies\"], maxlen = 9)\n",
        "  Y = data_for_model[\"first_party_won\"]\n",
        "  return X, I, Y"
      ],
      "metadata": {
        "id": "HMO3xTQZseeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the actual model"
      ],
      "metadata": {
        "id": "JmQ_Nmm2slwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(use_keras = True):\n",
        "  text_input = layers.Input(shape=(seq_length))\n",
        "  if (not use_keras):\n",
        "    embedding = layers.Embedding(max_features, embedding_dim, embeddings_initializer=keras.initializers.Constant(embedding_matrix), trainable=False)(text_input)\n",
        "  embedding = layers.Embedding(max_features, embedding_dim,input_length=seq_length)(text_input)\n",
        "  conv_layer_1 = layers.Conv1D(1024,8,padding=\"same\",activation=\"relu\")(embedding)\n",
        "  pool_1 = layers.MaxPool1D()(conv_layer_1)\n",
        "  norm_1 = layers.BatchNormalization()(pool_1)\n",
        "  conv_layer_2 = layers.Conv1D(1024,8,padding=\"same\",activation=\"relu\")(norm_1)\n",
        "  pool_2 = layers.MaxPool1D(pool_size=2)(conv_layer_2)\n",
        "  norm_2 = layers.BatchNormalization()(pool_2)\n",
        "  conv_layer_3 = layers.Conv1D(1024,8,padding=\"same\",activation=\"relu\")(norm_2)\n",
        "  text_pool = layers.GlobalAveragePooling1D()(conv_layer_3)\n",
        "\n",
        "  ideology_input = layers.Input(shape=(9))\n",
        "  i_1 = layers.Dense(100,activation=\"relu\")(ideology_input) \n",
        "  i_2 = layers.Dense(50,activation=\"relu\")(i_1)\n",
        "  i_3 = layers.Dense(20,activation=\"relu\")(i_2)  \n",
        "\n",
        "  \n",
        "  first_party_input = layers.Input(shape=(20,100))\n",
        "  fp_conv = layers.Conv1D(64,3,padding=\"same\",activation=\"relu\")(first_party_input)\n",
        "  fp_pool = layers.GlobalAvgPool1D()(fp_conv)\n",
        "\n",
        "  second_party_input = layers.Input(shape=(20,100))\n",
        "  s_conv = layers.Conv1D(64,3,padding=\"same\",activation=\"relu\")(second_party_input)\n",
        "  s_pool = layers.GlobalAvgPool1D()(s_conv)\n",
        "\n",
        "  #combined = layers.concatenate([text_pool,fp_pool,s_pool,i_pool])\n",
        "  combined = layers.concatenate([text_pool,i_3])\n",
        "\n",
        "  dense_1 = layers.Dense(1024,activation=\"relu\")(combined)\n",
        "  dense_2 = layers.Dense(128,activation=\"relu\")(dense_1)\n",
        "  norm_d = layers.BatchNormalization()(dense_2)\n",
        "  dense_3 = layers.Dense(16,activation=\"relu\")(norm_d)\n",
        "  output = layers.Dense(1,activation=\"sigmoid\")(dense_3)\n",
        "\n",
        "  #return keras.Model(inputs=(text_input,first_party_input,second_party_input,ideology_input),outputs=output)\n",
        "  #tf.metrics.BinaryAccuracy(threshold=0.0)\n",
        "  mod = keras.Model(inputs=(text_input, ideology_input),outputs=output)\n",
        "  mod.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  mod.summary()\n",
        "  return mod"
      ],
      "metadata": {
        "id": "YiPf-JiqsnQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = build_model(True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkf18krfs0v8",
        "outputId": "a92e520b-4912-4203-c05c-309b83aa3e57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 250)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 250, 100)     2000000     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 250, 1024)    820224      ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 125, 1024)   0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 125, 1024)   4096        ['max_pooling1d_1[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 125, 1024)    8389632     ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 62, 1024)    0           ['conv1d_3[0][0]']               \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 9)]          0           []                               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 62, 1024)    4096        ['max_pooling1d_2[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 100)          1000        ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 62, 1024)     8389632     ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 50)           5050        ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 1024)        0           ['conv1d_4[0][0]']               \n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 20)           1020        ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 1044)         0           ['global_average_pooling1d[0][0]'\n",
            "                                                                 , 'dense_2[0][0]']               \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 1024)         1070080     ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 128)          131200      ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 128)         512         ['dense_4[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 16)           2064        ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 1)            17          ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 20,818,623\n",
            "Trainable params: 20,814,271\n",
            "Non-trainable params: 4,352\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(m, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "jL1qzvxHs5Pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and test the model"
      ],
      "metadata": {
        "id": "YMQQq1Dtutqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(use_keras = True):\n",
        "  data, vec = load_data(use_keras)\n",
        "  print(\"loaded data\")\n",
        "  X, I, Y = get_complete_data()\n",
        "  print(\"vectorized data\")\n",
        "  model = build_model()\n",
        "  print(\"built model\")\n",
        "  if (not use_keras):\n",
        "    build_embedding(vec)\n",
        "    print(\"built embeddings\")\n",
        "  model.fit(x=(X,I), y = Y, batch_size=batch_size,epochs=10,validation_split=0.3)\n",
        "  #model.predict(x=[])\n",
        "  return model"
      ],
      "metadata": {
        "id": "zDGhq21Luxn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLUQaN493nGp",
        "outputId": "04a6debc-f42c-42d5-e29c-a6ce861ea1d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20000"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = train_model(use_keras = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "r1-ywMG9uzqg",
        "outputId": "7f476134-d485-4955-977b-3cd64dfb4508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded data\n",
            "vectorized data\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_10 (InputLayer)          [(None, 250)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, 250, 100)     2000000     ['input_10[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)             (None, 250, 1024)    820224      ['embedding_3[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling1d_5 (MaxPooling1D)  (None, 125, 1024)   0           ['conv1d_12[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 125, 1024)   4096        ['max_pooling1d_5[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)             (None, 125, 1024)    8389632     ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " max_pooling1d_6 (MaxPooling1D)  (None, 62, 1024)    0           ['conv1d_13[0][0]']              \n",
            "                                                                                                  \n",
            " input_11 (InputLayer)          [(None, 9)]          0           []                               \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 62, 1024)    4096        ['max_pooling1d_6[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 100)          1000        ['input_11[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)             (None, 62, 1024)     8389632     ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, 50)           5050        ['dense_14[0][0]']               \n",
            "                                                                                                  \n",
            " global_average_pooling1d_6 (Gl  (None, 1024)        0           ['conv1d_14[0][0]']              \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 20)           1020        ['dense_15[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 1044)         0           ['global_average_pooling1d_6[0][0\n",
            "                                                                 ]',                              \n",
            "                                                                  'dense_16[0][0]']               \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 1024)         1070080     ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " dense_18 (Dense)               (None, 128)          131200      ['dense_17[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 128)         512         ['dense_18[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_19 (Dense)               (None, 16)           2064        ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " dense_20 (Dense)               (None, 1)            17          ['dense_19[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 20,818,623\n",
            "Trainable params: 20,814,271\n",
            "Non-trainable params: 4,352\n",
            "__________________________________________________________________________________________________\n",
            "built model\n",
            "Epoch 1/10\n",
            "537/537 [==============================] - 642s 1s/step - loss: 0.6855 - accuracy: 0.6249 - val_loss: 0.7067 - val_accuracy: 0.6667\n",
            "Epoch 2/10\n",
            "537/537 [==============================] - 619s 1s/step - loss: 0.6622 - accuracy: 0.6445 - val_loss: 0.6690 - val_accuracy: 0.6634\n",
            "Epoch 3/10\n",
            "537/537 [==============================] - 622s 1s/step - loss: 0.6594 - accuracy: 0.6431 - val_loss: 0.6370 - val_accuracy: 0.6667\n",
            "Epoch 4/10\n",
            "537/537 [==============================] - 635s 1s/step - loss: 0.6570 - accuracy: 0.6454 - val_loss: 0.6487 - val_accuracy: 0.6667\n",
            "Epoch 5/10\n",
            " 46/537 [=>............................] - ETA: 9:28 - loss: 0.6437 - accuracy: 0.6576"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-600d4ea3628f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_keras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-365f6439e0e5>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(use_keras)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbuild_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"built embeddings\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0;31m#model.predict(x=[])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_for_model[\"ideologies\"].tolist()\n",
        "len(pad_sequences(data_for_model[\"ideologies\"].dropna(), maxlen = 9))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MwaQN0M0bO6",
        "outputId": "03b7439a-a3cc-4837-f17c-ef4313a53591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3067"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's try something else - BERT"
      ],
      "metadata": {
        "id": "56mdybOh4nhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT is a pretrained text model much like GPT, and it's used to predict words within sequences. However, it can be adapted for other purposes, like classifying text data."
      ],
      "metadata": {
        "id": "GGWuQSaf4rlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4jK1ZtoBJLM",
        "outputId": "5973e292-e720-4f47-bfb1-598dfac82615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 42.2 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 41.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 47.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
        "bert = TFAutoModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ONXmcew_gRQ",
        "outputId": "e2742a01-215c-4c3e-9045-8e96b29f05bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(use_keras = True):\n",
        "  text_input = layers.Input(shape=(seq_length))\n",
        "  if (not use_keras):\n",
        "    embedding = layers.Embedding(max_features, embedding_dim, embeddings_initializer=keras.initializers.Constant(embedding_matrix), trainable=False)(text_input)\n",
        "  embedding = layers.Embedding(max_features, embedding_dim,input_length=seq_length)(text_input)\n",
        "  conv_layer_1 = layers.Conv1D(1024,8,padding=\"same\",activation=\"relu\")(embedding)\n",
        "  pool_1 = layers.MaxPool1D()(conv_layer_1)\n",
        "  norm_1 = layers.BatchNormalization()(pool_1)\n",
        "  conv_layer_2 = layers.Conv1D(1024,8,padding=\"same\",activation=\"relu\")(norm_1)\n",
        "  pool_2 = layers.MaxPool1D(pool_size=2)(conv_layer_2)\n",
        "  norm_2 = layers.BatchNormalization()(pool_2)\n",
        "  conv_layer_3 = layers.Conv1D(1024,8,padding=\"same\",activation=\"relu\")(norm_2)\n",
        "  text_pool = layers.GlobalAveragePooling1D()(conv_layer_3)\n",
        "\n",
        "  ideology_input = layers.Input(shape=(9))\n",
        "  i_1 = layers.Dense(100,activation=\"relu\")(ideology_input) \n",
        "  i_2 = layers.Dense(50,activation=\"relu\")(i_1)\n",
        "  i_3 = layers.Dense(20,activation=\"relu\")(i_2)  \n",
        "\n",
        "  \n",
        "  first_party_input = layers.Input(shape=(20,100))\n",
        "  fp_conv = layers.Conv1D(64,3,padding=\"same\",activation=\"relu\")(first_party_input)\n",
        "  fp_pool = layers.GlobalAvgPool1D()(fp_conv)\n",
        "\n",
        "  second_party_input = layers.Input(shape=(20,100))\n",
        "  s_conv = layers.Conv1D(64,3,padding=\"same\",activation=\"relu\")(second_party_input)\n",
        "  s_pool = layers.GlobalAvgPool1D()(s_conv)\n",
        "\n",
        "  #combined = layers.concatenate([text_pool,fp_pool,s_pool,i_pool])\n",
        "  combined = layers.concatenate([text_pool,i_3])\n",
        "\n",
        "  dense_1 = layers.Dense(1024,activation=\"relu\")(combined)\n",
        "  dense_2 = layers.Dense(128,activation=\"relu\")(dense_1)\n",
        "  norm_d = layers.BatchNormalization()(dense_2)\n",
        "  dense_3 = layers.Dense(16,activation=\"relu\")(norm_d)\n",
        "  output = layers.Dense(1,activation=\"sigmoid\")(dense_3)\n",
        "\n",
        "  #return keras.Model(inputs=(text_input,first_party_input,second_party_input,ideology_input),outputs=output)\n",
        "  #tf.metrics.BinaryAccuracy(threshold=0.0)\n",
        "  mod = keras.Model(inputs=(text_input, ideology_input),outputs=output)\n",
        "  mod.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  mod.summary()\n",
        "  return mod"
      ],
      "metadata": {
        "id": "2TZZfyKMCn-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_layer = bert.layers.pop(0)"
      ],
      "metadata": {
        "id": "vGdNTTgHC40q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_layer"
      ],
      "metadata": {
        "id": "XVSldDHVDFvh",
        "outputId": "dcf884ab-154c-4bad-e8cb-df2577143bac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<transformers.models.bert.modeling_tf_bert.TFBertMainLayer at 0x7fb4737759d0>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    }
  ]
}